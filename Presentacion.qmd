---
title: "Algoritmo EM"
author: "Tamar Rojas Castillo"
format: 
  revealjs:
    incremental: true 
    scrollable: true
    logo: Logo Snoopy.png
---

## Preliminares {.smaller}

Supongamos que tenemos la Muestra 1 y la Muestra 2 cada una proveniente de una DistribuciÃ³n Normal $N(1,1)$ y $N(\mu,1)$ respectivamente

::: columns
::: {.column width="50%"}
$Muestra$ $1$ $=$ $(1, 2, x)$

-   Son extracciones independientes

<!-- -->

-   $x$ representa a un dato faltante

<!-- -->

-   Â¿Cual es la mejor suposiciÃ³n sobre el valor de x?

    -   El valor mÃ¡s probable de extraer de una $N(1,1)$ es $1$

    -   $x$ $=$ $1$
:::

::: {.column width="50%"}
$Muestra$ $2$ $=$ $(0,1,2)$

-   Todos los datos estan presentes

-   $\mu$ representa a un parÃ¡metro desconocido

-   Â¿Cual es el valor del parÃ¡metro que originÃ³ estos datos?

    -   Debido a que tenemos todos los datos de la muestra podemos estimar su valor con la media aritmÃ©tica

    -   $\mu$ $=$ $1$
:::
:::

## Â¿Que fuÃ© primero, el huevo o la gallina? {.smaller}

::: panel-tabset
## ProblemÃ¡tica

Supongamos que tenemos la siguiente muestra proveniente de una DistribuciÃ³n Normal $N(\mu,1)$ con una $Muestra$ $=$ $(1,2,x)$

-   Â¿Cual es la mejor suposicion para los valores de $($$x$, $\mu$ $)$ ?

    -   Necesito uno para poder obtener la estimaciÃ³n del otro

-   Caracteristicas:

    -   Variables latentes (faltantes que se estiman a partir de otras)

    -   ParÃ¡metros desconocidos

    -   observaciones en la muestra

-   Â¿Cual podria ser entonces un primer acercamiento a su resolucion?

    -Â¿MÃ©todo de la transformada inversa?

## EspÃ­ritu del algoritmo

Supongamos un valor inicial y arbitrario para un $\mu_{0}$ $=$ $0$ (es tambien valido partir desde un valor para $x$)

considerando $N(\mu,1)$ y $Muestra$ $=$ $(1,2,x)$

$\mu_{0}$ $=$ $0$ $\rightarrow$ $x_{0}$ $=$ $0$ (ya que es el mÃ¡s probable de extraer)

Luego si $x_{0}$ $=$ $0$ $\rightarrow$ $\mu_{1}$ $=$ $1$ $\rightarrow$ $x_{1}$ $=$ $1$ $\rightarrow$ $\mu_{2}$ $=$ $4/3$

$\vdots$

$\mu$ $=$ $\frac{1+2+x}{3}$ $\rightarrow$ $\mu$ $=$ $1.5$ $y$ $x=1.5$

## Convergencia EmpÃ­rica {.scrollable}

``` r
set.seed(0)
muestra <- c(1, 2, NA)  # Donde NA representa el valor desconocido de x
mu <- 0  # mu_0 inicial

# FunciÃ³n para actualizar mu y x iterativamente
Iteracion = function(muestra, max_iter = 20) {
  for (iter in 1:max_iter) { #funcion iter muy util :O
    # Actualizar el valor de x basado en el valor actual de mu 
    muestra[3] = mu  # Como el valor mÃ¡s probable es mu, asignamos x = mu
    
    # Calcular la nueva media mu basado en la muestra
    mu_nueva = mean(muestra)
    
    # Cat sirve para mostrar en ese orden
    cat("IteraciÃ³n:", iter, "x:", muestra[3],"mu:", mu_nueva, "\n")
    
    # Actualizar el valor de mu para la siguiente iteraciÃ³n
    mu <- mu_nueva
  }
  return(list(mu = mu, x = muestra[3]))
}


resultado = Iteracion(muestra)

# Mostrar el resultado final de mu y x
cat("Resultado final: x =", resultado$x, "mu =", resultado$mu, "\n")
```

```{r}

muestra <- c(1, 2, NA)  # Donde NA representa el valor desconocido de x
mu <- 0  # mu_0 inicial

# FunciÃ³n para actualizar mu y x iterativamente
Iteracion = function(muestra, max_iter = 20) {
  for (iter in 1:max_iter) { #funcion iter muy util :O
    # Actualizar el valor de x basado en el valor actual de mu 
    muestra[3] = mu  # Como el valor mÃ¡s probable es mu, asignamos x = mu
    
    # Calcular la nueva media mu basado en la muestra
    mu_nueva = mean(muestra)
    
    # Cat sirve para mostrar en ese orden
    cat("IteraciÃ³n:", iter, "x:", muestra[3],"mu:", mu_nueva, "\n")
    
    # Actualizar el valor de mu para la siguiente iteraciÃ³n
    mu <- mu_nueva
  }
  return(list(mu = mu, x = muestra[3]))
}


resultado = Iteracion(muestra)

# Mostrar el resultado final de mu y x
cat("Resultado final: x =", resultado$x, "mu =", resultado$mu, "\n")
```
:::

## Â¿Y si el algoritmo son los amigos que hicimos en el camino? {.smaller}

::: panel-tabset
## Pasos del algoritmo

-   **Paso 1: InicializaciÃ³n**

    -   $\mu_{0}$

-   ***Paso 2: Expectation step (E step)***

    -   Se calcula la esperanza condicional de la funciÃ³n objetivo, que generalmente es una funciÃ³n de log-verosimilitud. La idea es encontrar una estimaciÃ³n de los parÃ¡metros del modelo dado lo que se conoce (los datos observados) y dado un conjunto de valores actuales para los parÃ¡metros.

    -   La idea principal es utilizar la esperanza condicional para â€œcompletarâ€ los datos faltantes de manera probabilÃ­stica, lo que permite estimar los parÃ¡metros del modelo de manera mÃ¡s efectiva.

-   ***Paso 3: Maximization step (M step)***

    -   Se maximiza la esperanza condicional con respecto al parÃ¡metro objetivo. Se actualizan las estimaciones y se repiten iterativamente los pasos E y M hasta que el algoritmo converge segÃºn algÃºn criterio.

## MatemÃ¡tica

-   $P(x|\mu)\sim N(\mu,1)$

-   $\mathcal{L}(x,1,2|\mu)$ $=$ $P(x|\mu)\cdot P(1|\mu)\cdot P(2|\mu)$

    $\vdots$

    $log(P(x|\mu))$ + $log(P(1|\mu))$ + $log(P(2|\mu))$

-   $E(log(\mathcal{L}(x,1,2|\mu))$ $=$ $\int_{x}^{} P(x|\mu_{0})\cdot log(\mathcal{L}(x,1,2|\mu) dx$

-   teniendo $\mu_{1}$ pasamos al paso M que busca maximizar la funciÃ³n anterior

    $arg$ $max$ $E(log(\mathcal{L}(x,1,2|\mu))$
:::

## Ejemplo ðŸ¹ {.smaller}

``` r
set.seed(0) 

# Generar datos con valores faltantes o latentes
datos = rnorm(200, mean = 5, sd = 1) 
datos[160:200] <- NA #Aqui le asigno los NA

# Variables para datos observados y faltantes
datos_observados <- datos[!is.na(datos)] # Datos observados (sin NA), el "!" es la negaciÃ³n lÃ³gica
datos_faltantes <- datos[is.na(datos)] # Datos faltantes (NA)

# InicializaciÃ³n de valores
num_observados <- length(datos_observados) # NÃºmero de observaciones disponibles (basicamente es un subconjunto)
media_inicial <- mean(datos_observados) # Media de los datos observados
varianza_inicial <- var(datos_observados) * (num_observados - 1) / num_observados # Varianza ajustada



# FunciÃ³n EM para datos normales
em_norm <- function(datos, media_inicial, varianza_inicial) {
  datos_observados <- datos[!is.na(datos)]  # Valores observados
  num_total <- length(datos)  # Longitud total de datos
  num_observados <- length(datos_observados)  # NÃºmero de observaciones disponibles
  
  # FunciÃ³n para calcular la log-verosimilitud
  log_verosimilitud <- function(y, mu, sigma2) {
    -0.5 * length(y) * log(2 * pi * sigma2) - 0.5 * sum((y - mu)^2) / sigma2
  }
  
  # Calcular log-verosimilitud con los valores iniciales
  log_verosimilitud_anterior <- log_verosimilitud(datos_observados, media_inicial, varianza_inicial)
  
  
  
#Esta es la iteraciÃ³n del Algoritmo
  repeat {
    # E-Step: Esperanza condicional
    suma_esperada <- sum(datos_observados) + (num_total - num_observados) * media_inicial
    suma_cuadrados_esperada <- sum(datos_observados^2) + (num_total - num_observados) * (media_inicial^2 + varianza_inicial)
    
    # M-Step: ActualizaciÃ³n de parÃ¡metros
    media_actualizada <- suma_esperada / num_total
    varianza_actualizada <- suma_cuadrados_esperada / num_total - media_actualizada^2
    
    # Actualizar parÃ¡metros
    media_inicial <- media_actualizada
    varianza_inicial <- varianza_actualizada
    
    # Calcular log-verosimilitud con las nuevas estimaciones
    log_verosimilitud_actual <- log_verosimilitud(datos_observados, media_inicial, varianza_inicial)
    
    # Imprimir valores actuales de los parÃ¡metros y log-verosimilitud
    cat("Media estimada:", media_inicial, "Varianza estimada:", varianza_inicial, "Log-verosimilitud:", log_verosimilitud_actual, "\n")
    
    # Verificar convergencia
    if (abs(log_verosimilitud_anterior - log_verosimilitud_actual) < 0.0001) break
    
    # Actualizar log-verosimilitud anterior
    log_verosimilitud_anterior <- log_verosimilitud_actual
  }
  
  # Devolver los valores estimados de los parÃ¡metros
  return(c(media_estimada = media_inicial, varianza_estimada = varianza_inicial))
}

# Ejecutar la funciÃ³n EM con los valores iniciales
resultado <- em_norm(datos, media_inicial, varianza_inicial)
print(resultado)
```

```{r}
set.seed(0) 

# Generar datos con valores faltantes o latentes
datos = rnorm(200, mean = 5, sd = 1) 
datos[160:200] <- NA #Aqui le asigno los NA

# Variables para datos observados y faltantes
datos_observados <- datos[!is.na(datos)] # Datos observados (sin NA), el "!" es la negaciÃ³n lÃ³gica
datos_faltantes <- datos[is.na(datos)] # Datos faltantes (NA)

# InicializaciÃ³n de valores
num_observados <- length(datos_observados) # NÃºmero de observaciones disponibles (basicamente es un subconjunto)
media_inicial <- mean(datos_observados) # Media de los datos observados
varianza_inicial <- var(datos_observados) * (num_observados - 1) / num_observados # Varianza ajustada



# FunciÃ³n EM para datos normales
em_norm <- function(datos, media_inicial, varianza_inicial) {
  datos_observados <- datos[!is.na(datos)]  # Valores observados
  num_total <- length(datos)  # Longitud total de datos
  num_observados <- length(datos_observados)  # NÃºmero de observaciones disponibles
  
  # FunciÃ³n para calcular la log-verosimilitud
  log_verosimilitud <- function(y, mu, sigma2) {
    -0.5 * length(y) * log(2 * pi * sigma2) - 0.5 * sum((y - mu)^2) / sigma2
  }
  
  # Calcular log-verosimilitud con los valores iniciales
  log_verosimilitud_anterior <- log_verosimilitud(datos_observados, media_inicial, varianza_inicial)
  
  
  
#Esta es la iteraciÃ³n del Algoritmo
  repeat {
    # E-Step: Esperanza condicional
    suma_esperada <- sum(datos_observados) + (num_total - num_observados) * media_inicial
    suma_cuadrados_esperada <- sum(datos_observados^2) + (num_total - num_observados) * (media_inicial^2 + varianza_inicial)
    
    # M-Step: ActualizaciÃ³n de parÃ¡metros
    media_actualizada <- suma_esperada / num_total
    varianza_actualizada <- suma_cuadrados_esperada / num_total - media_actualizada^2
    
    # Actualizar parÃ¡metros
    media_inicial <- media_actualizada
    varianza_inicial <- varianza_actualizada
    
    # Calcular log-verosimilitud con las nuevas estimaciones
    log_verosimilitud_actual <- log_verosimilitud(datos_observados, media_inicial, varianza_inicial)
    
    # Imprimir valores actuales de los parÃ¡metros y log-verosimilitud
    cat("Media estimada:", media_inicial, "Varianza estimada:", varianza_inicial, "Log-verosimilitud:", log_verosimilitud_actual, "\n")
    
    # Verificar convergencia
    if (abs(log_verosimilitud_anterior - log_verosimilitud_actual) < 0.0001) break
    
    # Actualizar log-verosimilitud anterior
    log_verosimilitud_anterior <- log_verosimilitud_actual
  }
  
  # Devolver los valores estimados de los parÃ¡metros
  return(c(media_estimada = media_inicial, varianza_estimada = varianza_inicial))
}

# Ejecutar la funciÃ³n EM con los valores iniciales
resultado <- em_norm(datos, media_inicial, varianza_inicial)
print(resultado)
```

## Â¿En que otros problemas se ocupa? {.smaller}

-   En problemas con mÃ¡s de una DistribuciÃ³n

-   **Mixturas Gaussiana (Encontrar a que distribucion pertenecen los datos)**

    -   Â¿Se podrÃ­a trabajar como una Mezclas de Variables Aleatorias?

    -   Â¿Guarda relaciÃ³n con los mÃ©todos de composiciÃ³n? Spoiler... si!

-   Modelos ocultos de MÃ¡rkov (Determinar los parÃ¡metros desconocidos)

-   En problemas de optimizaciÃ³n de anÃ¡lisis numÃ©rico

    -   Tiempos de ejecuciÃ³n

    -   Convergencia

    -   Referencias

## Mixtura Gaussiana y library(mclust) {.smaller}

::: columns
::: {.column width="30%"}
``` r

library(mclust)  # Para ajuste de mezclas gaussianas
library(ggplot2) 
library(dplyr)  

set.seed(123)
n = 3000
mu1 = -3  
mu2 = 0   
mu3 = 3   
sigma1 = 0.5  
sigma2 = 1.0  
sigma3 = 0.7  

# Generar datos a partir de tres distribuciones gaussianas
x1 = rnorm(n/3, mean = mu1, sd = sigma1)
x2 = rnorm(n/3, mean = mu2, sd = sigma2)
x3 = rnorm(n/3, mean = mu3, sd = sigma3)


data <- c(x1, x2, x3)
df <- data.frame(data = data)

#Ajustar una mezcla de gaussianas usando el algoritmo EM
modelo_em = Mclust(data, G = 3)  # Especificar que queremos ajustar una mezcla de 3 gaussianas(G=K en la literatura)

#Obtener las densidades ajustadas de cada componente
densidades = data.frame(
  x = seq(min(data), max(data), length.out = 1000)
)
```
:::

::: {.column width="60%"}
![](ef3579f1-b342-4e74-8d37-41c1c8d5c103.png)
:::
:::

## Referencias {.smaller}

Rizzo, M. L. (2019). *Statistical computing with R, second edition* (2a ed.). Chapman & Hall/CRC. https://doi.org/10.1201/9780429192760

Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. *Journal of the Royal Statistical Society*, *39*(1), 1â€“38. http://www.jstor.org/stable/2984875

[Curso SimulaciÃ³n EstadÃ­stica](https://franplaza.github.io/courses/Simulation/Simulacion_2_Generando_variables_aleatorias.html#/m%C3%A9todo-de-composici%C3%B3n-sumas-y-mixturas "2024 -1s")
